# ğŸ” **EvaluaciÃ³n del Modelo**

## ğŸ“Œ DescripciÃ³n  
En esta seccion se evalua como funcionÃ³ el modelo despuÃ©s del entrenamiento. TambiÃ©n comparamos los resultados con otros enfoques y explicamos por quÃ© elegimos este modelo.  

## ğŸ“Š **Resultados de EvaluaciÃ³n**  
DespuÃ©s de las pruebas, estos fueron las metricas calculadas:  

- **mAP@0.5:** **0.6902** â†’ QuÃ© tan bien detecta los objetos con un umbral de IoU del 50%.  
- **mAP@0.5:0.95:** **0.6725** â†’ Similar al anterior, pero considerando umbrales mÃ¡s exigentes.  
- **PrecisiÃ³n:** **94.29%** â†’ De todas las detecciones, este porcentaje fue correcto.  
- **Recall:** **92.56%** â†’ De todos los objetos reales, el modelo encontrÃ³ este porcentaje.  
- **F1-score:** **0.9341** â†’ Un balance entre precisiÃ³n y recall, indicando un buen desempeÃ±o general.  

## ğŸ“Œ **Â¿QuÃ© significan estas metricas para el modelo?**  
âœ… **Buen equilibrio entre precisiÃ³n y recall** â†’ El modelo hace bien su trabajo sin ser demasiado estricto ni demasiado permisivo.  
âœ… **F1-score sÃ³lido** â†’ La relaciÃ³n entre errores y aciertos es bastante estable.  
âœ… **El mAP es bueno, pero puede mejorar** â†’ Aunque los valores son altos, hay margen para optimizar la detecciÃ³n en situaciones mÃ¡s difÃ­ciles.  

### âœ… **En resumen:**  
El modelo estÃ¡ funcionando bastante bien, pero con algunos ajustes podemos hacerlo aÃºn mÃ¡s preciso y confiable. ğŸš€  
